{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce233ca",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea72a4f",
   "metadata": {},
   "source": [
    "https://nn.labml.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcfa360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9138979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9f0329",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc83eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd473b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3894,  1.8079, -0.4922,  ...,  0.4141,  0.7742, -1.3017],\n",
       "         [ 0.5475, -0.6181,  0.4773,  ...,  0.4435,  0.0295,  0.9851],\n",
       "         [-0.5884,  0.3026, -0.5085,  ..., -0.2530,  1.3438, -0.5324],\n",
       "         [ 1.8149,  0.8070,  0.2444,  ...,  0.9461, -1.0576,  1.7348]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c016d984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1536, bias=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
    "qkv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b41035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7449, -0.1229, -0.5324,  ..., -0.2585, -0.3609,  0.8873],\n",
       "         [ 0.3557, -0.1613, -0.5249,  ...,  0.0855,  0.0497,  0.1466],\n",
       "         [-0.3019,  0.3314, -0.1162,  ...,  0.3522, -0.4374,  0.2474],\n",
       "         [ 0.2902, -0.0559, -0.0318,  ..., -0.2233,  0.5550, -1.1974]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv_layer(x)\n",
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb63ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f0aaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.7449e+00, -1.2295e-01, -5.3236e-01,  ..., -1.8445e-01,\n",
       "            1.2201e-01,  6.9133e-01],\n",
       "          [ 1.7298e-01,  2.5862e-01, -5.3273e-01,  ..., -5.8945e-01,\n",
       "            5.6178e-01,  3.4953e-01],\n",
       "          [-1.3525e-01,  8.2513e-01, -7.3156e-01,  ...,  8.5626e-02,\n",
       "           -7.6213e-02, -7.1592e-01],\n",
       "          ...,\n",
       "          [-5.4403e-01, -3.5871e-01,  8.7709e-01,  ...,  4.3994e-01,\n",
       "            6.0564e-01, -1.0475e+00],\n",
       "          [ 5.3648e-01, -2.6786e-01,  1.4037e-01,  ..., -1.6509e-01,\n",
       "           -3.5366e-01, -7.4077e-01],\n",
       "          [-2.8201e-01, -5.6094e-01, -6.3094e-01,  ..., -2.5846e-01,\n",
       "           -3.6089e-01,  8.8730e-01]],\n",
       "\n",
       "         [[ 3.5571e-01, -1.6133e-01, -5.2491e-01,  ..., -3.4181e-01,\n",
       "            4.4692e-01,  7.6768e-01],\n",
       "          [-4.2524e-01,  7.3394e-01, -7.7988e-02,  ..., -7.8987e-01,\n",
       "           -1.2612e-02, -2.1245e-02],\n",
       "          [-3.3983e-01,  5.3461e-01, -1.5202e-01,  ..., -2.1632e-01,\n",
       "            3.1865e-01,  8.4330e-01],\n",
       "          ...,\n",
       "          [ 1.2923e+00, -3.5355e-01, -3.1021e-01,  ...,  2.1127e-01,\n",
       "            4.2763e-01,  3.0975e-01],\n",
       "          [-3.6354e-01, -4.7865e-02,  3.5701e-01,  ...,  2.9729e-01,\n",
       "            1.0329e+00,  3.2266e-01],\n",
       "          [ 6.6894e-02,  5.6538e-01,  1.3847e-01,  ...,  8.5464e-02,\n",
       "            4.9739e-02,  1.4663e-01]],\n",
       "\n",
       "         [[-3.0191e-01,  3.3140e-01, -1.1618e-01,  ..., -5.7576e-02,\n",
       "            9.1675e-01, -2.4070e-01],\n",
       "          [ 2.8131e-02,  8.2792e-04, -3.5391e-02,  ..., -4.1557e-01,\n",
       "            3.3043e-01, -4.8743e-01],\n",
       "          [ 2.6579e-01,  3.3426e-01, -2.1708e-01,  ..., -1.7211e-01,\n",
       "            5.1269e-01,  9.2016e-01],\n",
       "          ...,\n",
       "          [ 3.4162e-01, -1.8505e-01,  7.6732e-01,  ..., -3.2037e-01,\n",
       "            1.0667e+00,  1.6684e+00],\n",
       "          [-5.3805e-01, -2.2428e-01, -7.9426e-01,  ...,  4.8363e-01,\n",
       "            1.3980e+00,  1.4516e-01],\n",
       "          [ 8.1107e-01, -9.1963e-01,  2.1280e-01,  ...,  3.5217e-01,\n",
       "           -4.3738e-01,  2.4736e-01]],\n",
       "\n",
       "         [[ 2.9015e-01, -5.5929e-02, -3.1805e-02,  ...,  5.1545e-01,\n",
       "           -2.4088e-01, -1.4840e+00],\n",
       "          [ 1.5258e-01,  4.3358e-01, -7.5993e-01,  ..., -1.0307e+00,\n",
       "           -2.1064e-01, -1.0382e-01],\n",
       "          [ 3.3989e-01, -5.9438e-01,  8.3139e-01,  ...,  2.2856e-01,\n",
       "            5.1001e-01,  1.5199e-01],\n",
       "          ...,\n",
       "          [-2.1808e-01, -2.8797e-01,  7.4222e-01,  ..., -1.1519e+00,\n",
       "           -6.4138e-01,  3.1553e-01],\n",
       "          [ 2.8662e-02, -1.2251e-01, -1.1402e-01,  ..., -1.9914e-01,\n",
       "            1.5470e-01,  1.9902e-02],\n",
       "          [-2.5699e-01, -2.1533e-01, -1.1033e-01,  ..., -2.2330e-01,\n",
       "            5.5499e-01, -1.1974e+00]]]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a6469d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5cfd531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e32e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9c80033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "033b7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\torch2tf2\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3281.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 8, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8eaa72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6494,  0.0590],\n",
       "        [-0.1545, -0.8527],\n",
       "        [ 1.1263,  1.5299]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(2, 3)\n",
    "torch.transpose(y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbca6571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True]],\n",
       "\n",
       "         [[True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          ...,\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True],\n",
       "          [True, True, True, True]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-1, -2) == k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d30558d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcfe37d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size() , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask[0][1] # mask for input to a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf8dd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2927,    -inf,    -inf,    -inf],\n",
       "        [ 0.1736, -0.5931,    -inf,    -inf],\n",
       "        [-0.0556, -0.0105,  0.1946,    -inf],\n",
       "        [ 0.3081, -0.1581,  0.2623, -0.1884]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb860ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled += mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ef9f55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6269606805367254"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.5596) / (np.exp(0.5596) + np.exp(0.0404))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33f044a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(scaled, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc53d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54405030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention, v)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ee86d",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0ab9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "076b52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = scaled_dot_product(q, k, v, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77007aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da872927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6828, 0.3172, 0.0000, 0.0000],\n",
       "        [0.3003, 0.3141, 0.3856, 0.0000],\n",
       "        [0.3134, 0.1966, 0.2993, 0.1907]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1045f31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ba3dece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n",
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63be9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "503c1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = linear_layer(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa2a2aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17ff9f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0141, -0.1967,  0.1734,  ..., -0.1613, -0.1859, -0.2678],\n",
       "         [ 0.1112, -0.0805, -0.1931,  ...,  0.0825,  0.0424, -0.2536],\n",
       "         [-0.0491, -0.0923,  0.1491,  ...,  0.1260,  0.1025,  0.0531],\n",
       "         [-0.5752, -0.2584,  0.2120,  ..., -0.0178,  0.0558, -0.2604]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bc5ae",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62efe454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300820c",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09050425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516343f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
